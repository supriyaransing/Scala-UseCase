{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d66133f",
   "metadata": {},
   "source": [
    "### Q1) Create first dataframe having values as given below:\n",
    "\n",
    "Roll,Name,Section\n",
    "\n",
    "12,Manoj,A\n",
    "\n",
    "10,Pankaj,B\n",
    "\n",
    "14,Maya,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da99877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LIN24011373.corp.capgemini.com:4040\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1649343713760)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca7b12d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb4181bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c50e7ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|Roll|  Name|Section|\n",
      "+----+------+-------+\n",
      "|  12| Manoj|      A|\n",
      "|  10|Pankaj|      B|\n",
      "|  14|  Maya|      A|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Roll: int, Name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data=Seq((12,\"Manoj\",\"A\"),(10,\"Pankaj\",\"B\"),(14,\"Maya\",\"A\")).toDF(\"Roll\",\"Name\",\"Section\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af073dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll: integer (nullable = false)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53148ea7",
   "metadata": {},
   "source": [
    "#### Create a second dataframe having values as given below:\n",
    "Roll,Subject,Marks,Grade\n",
    "\n",
    "22,Maths,79,B\n",
    "\n",
    "23,Physics,98,A\n",
    "\n",
    "24,Chemistry,60,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a49d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+-----+\n",
      "|Roll|  Subject|Marks|Grade|\n",
      "+----+---------+-----+-----+\n",
      "|  22|    Maths|   79|    B|\n",
      "|  23|  Physics|   98|    A|\n",
      "|  24|Chemistry|   60|    C|\n",
      "+----+---------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data1: org.apache.spark.sql.DataFrame = [Roll: int, Subject: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1=Seq((22,\"Maths\",79,\"B\"),(23,\"Physics\",98,\"A\"),(24,\"Chemistry\",60,\"C\")).toDF(\"Roll\",\"Subject\",\"Marks\",\"Grade\")\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd0ee07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll: integer (nullable = false)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Marks: integer (nullable = false)\n",
      " |-- Grade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12b460",
   "metadata": {},
   "source": [
    "//For the above given two given datasets, perform the following operations.\n",
    "\n",
    "### a)Load above given two datasets in two different List and convert that List to a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "487ae906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list1: List[org.apache.spark.sql.Row] = List([12,Manoj,A], [10,Pankaj,B], [14,Maya,A])\r\n",
       "list2: List[org.apache.spark.sql.Row] = List([22,Maths,79,B], [23,Physics,98,A], [24,Chemistry,60,C])\r\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var list1=data.collect().toList\n",
    "var list2=data1.collect().toList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddcc95",
   "metadata": {},
   "source": [
    "##### Converts lists into dataframe using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5c3b80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[176] at parallelize at <console>:58\r\n",
       "new_schema: org.apache.spark.sql.types.StructType = StructType(StructField(Roll,IntegerType,true), StructField(Name,StringType,true), StructField(Section,StringType,true))\r\n",
       "df_list: org.apache.spark.sql.DataFrame = [Roll: int, Name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_rdd=spark.sparkContext.parallelize(list1)\n",
    "val new_schema = StructType(Array(\n",
    "  StructField(\"Roll\", IntegerType, true),\n",
    "  StructField(\"Name\", StringType, true),\n",
    "  StructField(\"Section\", StringType, true)\n",
    "))\n",
    "val df_list=spark.createDataFrame(new_rdd,new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8d21130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|Roll|  Name|Section|\n",
      "+----+------+-------+\n",
      "|  12| Manoj|      A|\n",
      "|  10|Pankaj|      B|\n",
      "|  14|  Maya|      A|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4688d4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_rdd1: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[180] at parallelize at <console>:58\r\n",
       "new_schema1: org.apache.spark.sql.types.StructType = StructType(StructField(roll,IntegerType,true), StructField(subject,StringType,true), StructField(marks,IntegerType,true))\r\n",
       "df_list1: org.apache.spark.sql.DataFrame = [roll: int, subject: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_rdd1=spark.sparkContext.parallelize(list2)\n",
    "val new_schema1 = StructType(Array(\n",
    "  StructField(\"roll\", IntegerType, true),\n",
    "  StructField(\"subject\", StringType, true),\n",
    "  StructField(\"marks\", IntegerType, true),\n",
    "))\n",
    "val df_list1=spark.createDataFrame(new_rdd1,new_schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50a62c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: org.apache.spark.sql.DataFrame = [roll: int, subject: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da511a",
   "metadata": {},
   "source": [
    "#### b)create a generalised scala function to perform the union of two created dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da84f2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_union: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Roll: int, Name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_union=df_list.union(df_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa2fb7",
   "metadata": {},
   "source": [
    "#### c)Display both individual and union datasets results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbec4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|Roll|  Name|Section|\n",
      "+----+------+-------+\n",
      "|  12| Manoj|      A|\n",
      "|  10|Pankaj|      B|\n",
      "|  14|  Maya|      A|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7e0879bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+\n",
      "|roll|  subject|marks|\n",
      "+----+---------+-----+\n",
      "|  22|    Maths|   79|\n",
      "|  23|  Physics|   98|\n",
      "|  24|Chemistry|   60|\n",
      "+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dbd1231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+\n",
      "|Roll|     Name|Section|\n",
      "+----+---------+-------+\n",
      "|  12|    Manoj|      A|\n",
      "|  10|   Pankaj|      B|\n",
      "|  14|     Maya|      A|\n",
      "|  22|    Maths|     79|\n",
      "|  23|  Physics|     98|\n",
      "|  24|Chemistry|     60|\n",
      "+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016de56",
   "metadata": {},
   "source": [
    "### Q2) Given below is the books table.\n",
    "\n",
    "Author      BookName    CostPerUnit UnitsSold\n",
    "Author1\t   HarryPorter     1000       56\n",
    "Author2    Untold Story    658        30\n",
    "Author3    ThreeIdiots     500        60\n",
    "Author2    Mind-Master     488        45\n",
    "Author2    CourtsofIndia   560        60\n",
    "Author4      Manav         560        45\n",
    "Author3    SaffronSwords   490        50\n",
    "\n",
    "#### For the above given datasets, perform the following operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117ef85",
   "metadata": {},
   "source": [
    "### a)Expect data is having delimiter as comma. Load dataset in a List and convert that List to a dataframe say data_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c56ca91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows_df: Seq[org.apache.spark.sql.Row] = List([Author1,HarryPorter,1000,56], [Author2,Untold Story,658,30], [Author3,ThreeIdiots,500,60], [Author2,Mind-Master,488,45], [Author2,CourtsofIndia,560,60], [Author4,Manav,560,45], [Author3,SaffronSwords,490,50])\r\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rows_df=Seq(Row(\"Author1\",\"HarryPorter\",1000,56),\n",
    "                Row(\"Author2\",\"Untold Story\",658,30),\n",
    "                Row(\"Author3\",\"ThreeIdiots\",500,60),\n",
    "                Row(\"Author2\",\"Mind-Master\",488,45),\n",
    "                Row(\"Author2\",\"CourtsofIndia\",560,60),\n",
    "                Row(\"Author4\",\"Manav\",560,45),\n",
    "                Row(\"Author3\",\"SaffronSwords\",490,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e68c6",
   "metadata": {},
   "source": [
    "#### Load dataset into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d61440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema_df: List[org.apache.spark.sql.types.StructField] = List(StructField(Author,StringType,true), StructField(BookName,StringType,true), StructField(CostPerUnit,IntegerType,true), StructField(UnitsSold,IntegerType,true))\r\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema_df=List(StructField(\"Author\",StringType,true),\n",
    "                   StructField(\"BookName\",StringType,true),\n",
    "                   StructField(\"CostPerUnit\",IntegerType,true),\n",
    "                   StructField(\"UnitsSold\",IntegerType,true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fd47c",
   "metadata": {},
   "source": [
    "#### Converts that list into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d172df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_df: org.apache.spark.sql.DataFrame = [Author: string, BookName: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_df=spark.createDataFrame(spark.sparkContext.parallelize(rows_df),StructType(schema_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d38a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+---------+\n",
      "| Author|     BookName|CostPerUnit|UnitsSold|\n",
      "+-------+-------------+-----------+---------+\n",
      "|Author1|  HarryPorter|       1000|       56|\n",
      "|Author2| Untold Story|        658|       30|\n",
      "|Author3|  ThreeIdiots|        500|       60|\n",
      "|Author2|  Mind-Master|        488|       45|\n",
      "|Author2|CourtsofIndia|        560|       60|\n",
      "|Author4|        Manav|        560|       45|\n",
      "|Author3|SaffronSwords|        490|       50|\n",
      "+-------+-------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e67dd7",
   "metadata": {},
   "source": [
    "### b)Create a column named Earning and find the total cost earned by each book based on CostPerUnit and UnitsSold by creating a Scala UDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2972a86",
   "metadata": {},
   "source": [
    "##### UDFâ€™s are used to extend the functions of the framework and re-use this function on several DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17117805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_cal: (costperunit: Int, unitssold: Int)Int\r\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Created a Function\n",
    "\n",
    "def total_cal(costperunit:Int,unitssold:Int):Int={\n",
    "    val total_cost=costperunit*unitssold\n",
    "    return total_cost\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d4c24e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\r\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " //this function is available at org.apache.spark.sql.functions.udf package. \n",
    "//import this package before using it.\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7902ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4751/0x00000001017e7040@69b3911c,IntegerType,List(Some(class[value[0]: int]), Some(class[value[0]: int])),Some(class[value[0]: int]),None,false,true)\r\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//convert this function total_cal() to UDF by passing the function to Spark SQL udf(),\n",
    "\n",
    "val totalUDF=udf(total_cal(_,_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9ccbbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_df1: org.apache.spark.sql.DataFrame = [Author: string, BookName: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// New Column - Earning\n",
    "\n",
    "val data_df1=data_df.withColumn(\"Earning\",totalUDF('CostPerUnit,'UnitsSold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e56ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+---------+-------+\n",
      "| Author|     BookName|CostPerUnit|UnitsSold|Earning|\n",
      "+-------+-------------+-----------+---------+-------+\n",
      "|Author1|  HarryPorter|       1000|       56|  56000|\n",
      "|Author2| Untold Story|        658|       30|  19740|\n",
      "|Author3|  ThreeIdiots|        500|       60|  30000|\n",
      "|Author2|  Mind-Master|        488|       45|  21960|\n",
      "|Author2|CourtsofIndia|        560|       60|  33600|\n",
      "|Author4|        Manav|        560|       45|  25200|\n",
      "|Author3|SaffronSwords|        490|       50|  24500|\n",
      "+-------+-------------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf3c8c",
   "metadata": {},
   "source": [
    "**c)Find the best top two selling books of each Author using ranking logic. Make use of scala in dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4cddc534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.rank\r\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0c187b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4e182f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top: org.apache.spark.sql.DataFrame = [Author: string, BookName: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var top=data_df1.withColumn(\"rank\",rank().over(Window.partitionBy($\"Author\").orderBy($\"UnitsSold\".desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "05655779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+---------+-------+----+\n",
      "| Author|     BookName|CostPerUnit|UnitsSold|Earning|rank|\n",
      "+-------+-------------+-----------+---------+-------+----+\n",
      "|Author1|  HarryPorter|       1000|       56|  56000|   1|\n",
      "|Author2|CourtsofIndia|        560|       60|  33600|   1|\n",
      "|Author2|  Mind-Master|        488|       45|  21960|   2|\n",
      "|Author2| Untold Story|        658|       30|  19740|   3|\n",
      "|Author3|  ThreeIdiots|        500|       60|  30000|   1|\n",
      "|Author3|SaffronSwords|        490|       50|  24500|   2|\n",
      "|Author4|        Manav|        560|       45|  25200|   1|\n",
      "+-------+-------------+-----------+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691920c4",
   "metadata": {},
   "source": [
    "**Q3)**\n",
    "Create a file named result.json file with the following data.\n",
    "{\"Name\":\"Manoj\",\"status\":\"PASS\",\"Comments\": \"has good conduct in class\",\"Subjects\":[{\"sub\":\"Maths\",\"Marks\":98},{\"sub\":\"Physics\",\"Marks\":86},{\"sub\":\"Chemistry\",\"Marks\":88},{\"sub\":\"Biology\",\"Marks\":72},{\"sub\":\"Computer\",\"Marks\":90}]}\n",
    "\n",
    "Perform the following task using scala spark with above given data and without creating any manual structure schema of json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244743c",
   "metadata": {},
   "source": [
    "### a) Extract Subjects values in separate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f8a228f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.explode\r\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{explode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "204b35a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = C:\\Users\\SUPBHAUS\\Desktop\\Scala Training\\Result.json\r\n",
       "json_data: org.apache.spark.sql.DataFrame = [Comments: string, Name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"C:\\\\Users\\\\SUPBHAUS\\\\Desktop\\\\Scala Training\\\\Result.json\"\n",
    "val json_data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b7a282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Comments: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Subjects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Marks: long (nullable = true)\n",
      " |    |    |-- sub: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4d23da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------+---------------+\n",
      "|            Comments| Name|            Subjects|status|       sub_flat|\n",
      "+--------------------+-----+--------------------+------+---------------+\n",
      "|has good conduct ...|Manoj|[{98, Maths}, {86...|  PASS|    {98, Maths}|\n",
      "|has good conduct ...|Manoj|[{98, Maths}, {86...|  PASS|  {86, Physics}|\n",
      "|has good conduct ...|Manoj|[{98, Maths}, {86...|  PASS|{88, Chemistry}|\n",
      "|has good conduct ...|Manoj|[{98, Maths}, {86...|  PASS|  {72, Biology}|\n",
      "|has good conduct ...|Manoj|[{98, Maths}, {86...|  PASS| {90, Computer}|\n",
      "+--------------------+-----+--------------------+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Comments: string, Name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = json_data.withColumn(\"sub_flat\",explode($\"Subjects\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4fee1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       sub_flat|\n",
      "+---------------+\n",
      "|    {98, Maths}|\n",
      "|  {86, Physics}|\n",
      "|{88, Chemistry}|\n",
      "|  {72, Biology}|\n",
      "| {90, Computer}|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"sub_flat\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3615192",
   "metadata": {},
   "source": [
    "### b) Extract the values as sub column and their corresponding marks as marks column and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9faaaf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sub_col: org.apache.spark.sql.DataFrame = [marks: bigint]\r\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sub_col=df.select(col(\"sub_flat.marks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "705e1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|marks|\n",
      "+-----+\n",
      "|   98|\n",
      "|   86|\n",
      "|   88|\n",
      "|   72|\n",
      "|   90|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc9539",
   "metadata": {},
   "source": [
    "### c) Create a column say grade and assign grade to each subject based on the marks obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f0e22739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sub_col1: org.apache.spark.sql.DataFrame = [Marks: bigint, sub: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sub_col1=sub_col.withColumn(\"grade\",when(col(\"Marks\")>=90,\"A\").when(col(\"Marks\")>80 && col(\"Marks\")<90,\"B\").otherwise(\"C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "08b3ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+\n",
      "|Marks|      sub|grade|\n",
      "+-----+---------+-----+\n",
      "|   98|    Maths|    A|\n",
      "|   86|  Physics|    B|\n",
      "|   88|Chemistry|    B|\n",
      "|   72|  Biology|    C|\n",
      "|   90| Computer|    A|\n",
      "+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_col1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda3877",
   "metadata": {},
   "source": [
    "###\n",
    "**Q4)** Given below is a dataset which will be stored in a csv file named employee.csv\n",
    "1,Amit,1000,01/01/2016\n",
    "1,Amit,2000,02/01/2016\n",
    "1,Amit,1000,03/01/2016\n",
    "1,Amit,2000,04/01/2016\n",
    "1,Amit,3000,05/01/2016\n",
    "1,Amit,1000,06/01/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f37e2",
   "metadata": {},
   "source": [
    "### a) Define a schema for the file and load the data in a dataframe using defined schema. Create a new column named prevSal which will have data of Amit's previous row value of salary as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f2026948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema3: List[org.apache.spark.sql.types.StructField] = List(StructField(EmpId,IntegerType,true), StructField(EmpName,StringType,true), StructField(Salary,IntegerType,true), StructField(SalaryDate,StringType,true))\r\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema3=List(StructField(\"EmpId\",IntegerType,true),\n",
    "                 StructField(\"EmpName\",StringType,true),\n",
    "                 StructField(\"Salary\",IntegerType,true),\n",
    "                 StructField(\"SalaryDate\",StringType,true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5fa6e711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d69faa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_df: org.apache.spark.sql.DataFrame = [EmpId: int, EmpName: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emp_df=spark.read.option(\"header\",\"true\").schema(StructType(schema3)).format(\"CSV\").load(\"C:\\\\Users\\\\SUPBHAUS\\\\Desktop\\\\Scala Training\\\\employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "278aae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+\n",
      "|EmpId|EmpName|Salary|SalaryDate|\n",
      "+-----+-------+------+----------+\n",
      "|    1|   Amit|  1000|01-01-2016|\n",
      "|    1|   Amit|  2000|02-01-2016|\n",
      "|    1|   Amit|  1000|03-01-2016|\n",
      "|    1|   Amit|  2000|04-01-2016|\n",
      "|    1|   Amit|  3000|05-01-2016|\n",
      "|    1|   Amit|  1000|06-01-2016|\n",
      "|    1|   Amit|  2000|07-01-2016|\n",
      "+-----+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd4e384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "window: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@615148e7\r\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//order by Salary Date to get previous salary.\n",
    "\n",
    "val window=Window.orderBy(\"SalaryDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7791910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_df1: org.apache.spark.sql.DataFrame = [EmpId: int, EmpName: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//use lag to get previous row value for salary, 1 is the offset\n",
    "//For first row we will get NULl\n",
    "\n",
    "val emp_df1=emp_df.withColumn(\"prevSal\",lag(col(\"Salary\"),1).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "70c90a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+-------+\n",
      "|EmpId|EmpName|Salary|SalaryDate|prevSal|\n",
      "+-----+-------+------+----------+-------+\n",
      "|    1|   Amit|  1000|01-01-2016|   null|\n",
      "|    1|   Amit|  2000|02-01-2016|   1000|\n",
      "|    1|   Amit|  1000|03-01-2016|   2000|\n",
      "|    1|   Amit|  2000|04-01-2016|   1000|\n",
      "|    1|   Amit|  3000|05-01-2016|   2000|\n",
      "|    1|   Amit|  1000|06-01-2016|   3000|\n",
      "|    1|   Amit|  2000|07-01-2016|   1000|\n",
      "+-----+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e8572",
   "metadata": {},
   "source": [
    "###b) Create a new column named NextSal which will have data of Amit's next row value of salary as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "09156084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_df2: org.apache.spark.sql.DataFrame = [EmpId: int, EmpName: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//useful when we have usecases like comparison with next value. \n",
    "\n",
    "//order by Salary Date to get previous salary. \n",
    "//or first row we will get NULL\n",
    "\n",
    "val emp_df2=emp_df1.withColumn(\"NextSal\",lead(col(\"Salary\"),1).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "31bfc8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+-------+-------+\n",
      "|EmpId|EmpName|Salary|SalaryDate|prevSal|NextSal|\n",
      "+-----+-------+------+----------+-------+-------+\n",
      "|    1|   Amit|  1000|01-01-2016|   null|   2000|\n",
      "|    1|   Amit|  2000|02-01-2016|   1000|   1000|\n",
      "|    1|   Amit|  1000|03-01-2016|   2000|   2000|\n",
      "|    1|   Amit|  2000|04-01-2016|   1000|   3000|\n",
      "|    1|   Amit|  3000|05-01-2016|   2000|   1000|\n",
      "|    1|   Amit|  1000|06-01-2016|   3000|   2000|\n",
      "|    1|   Amit|  2000|07-01-2016|   1000|   null|\n",
      "+-----+-------+------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8c11b",
   "metadata": {},
   "source": [
    "###c) Create a new column as salDiff having difference of column nextSal and prevSal .Create one more new column as increment. If the difference is null then increment value will be same. If its greater than 0 then up else down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a13a82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_df3: org.apache.spark.sql.DataFrame = [EmpId: int, EmpName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emp_df3=emp_df2.withColumn(\"salDiff\",col(\"NextSal\")-col(\"prevSal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b422b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+-------+-------+-------+\n",
      "|EmpId|EmpName|Salary|SalaryDate|prevSal|NextSal|salDiff|\n",
      "+-----+-------+------+----------+-------+-------+-------+\n",
      "|    1|   Amit|  1000|01-01-2016|   null|   2000|   null|\n",
      "|    1|   Amit|  2000|02-01-2016|   1000|   1000|      0|\n",
      "|    1|   Amit|  1000|03-01-2016|   2000|   2000|      0|\n",
      "|    1|   Amit|  2000|04-01-2016|   1000|   3000|   2000|\n",
      "|    1|   Amit|  3000|05-01-2016|   2000|   1000|  -1000|\n",
      "|    1|   Amit|  1000|06-01-2016|   3000|   2000|  -1000|\n",
      "|    1|   Amit|  2000|07-01-2016|   1000|   null|   null|\n",
      "+-----+-------+------+----------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e3d16e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_df4: org.apache.spark.sql.DataFrame = [EmpId: int, EmpName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emp_df4=emp_df3.withColumn(\"increment\",when(col(\"salDiff\").isNull,\"Same\").when(col(\"salDiff\")<=0,\"Down\").when(col(\"salDiff\")>0,\"Up\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0c3d1466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+-------+-------+-------+---------+\n",
      "|EmpId|EmpName|Salary|SalaryDate|prevSal|NextSal|salDiff|increment|\n",
      "+-----+-------+------+----------+-------+-------+-------+---------+\n",
      "|    1|   Amit|  1000|01-01-2016|   null|   2000|   null|     Same|\n",
      "|    1|   Amit|  2000|02-01-2016|   1000|   1000|      0|     Down|\n",
      "|    1|   Amit|  1000|03-01-2016|   2000|   2000|      0|     Down|\n",
      "|    1|   Amit|  2000|04-01-2016|   1000|   3000|   2000|       Up|\n",
      "|    1|   Amit|  3000|05-01-2016|   2000|   1000|  -1000|     Down|\n",
      "|    1|   Amit|  1000|06-01-2016|   3000|   2000|  -1000|     Down|\n",
      "|    1|   Amit|  2000|07-01-2016|   1000|   null|   null|     Same|\n",
      "+-----+-------+------+----------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b95a9d",
   "metadata": {},
   "source": [
    "### Q5) Derive relevant scenarios of different types of joins using spark scala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf4eb2",
   "metadata": {},
   "source": [
    "The Spark SQL supports several types of joins such as inner join, cross join, left outer join, right outer join, full outer join, left semi-join, left anti join. \n",
    "Consider following two datasets:\n",
    "\n",
    "1.Book Dataset\n",
    "2.Writer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40161392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|writer_name|writer_id|\n",
      "+-----------+---------+\n",
      "|     Martin|        1|\n",
      "|    Zaharia|        2|\n",
      "|       Neha|        3|\n",
      "|      James|        4|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "writerdata: Seq[(String, Int)] = List((Martin,1), (Zaharia,2), (Neha,3), (James,4))\r\n",
       "writer_df: org.apache.spark.sql.DataFrame = [writer_name: string, writer_id: int]\r\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val writerdata = Seq((\"Martin\",1),(\"Zaharia\", 2),(\"Neha\", 3),(\"James\", 4))\n",
    "val writer_df = writerdata.toDF(\"writer_name\",\"writer_id\")\n",
    "writer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cda65334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+\n",
      "|book_name|cost|writer_id|\n",
      "+---------+----+---------+\n",
      "|    Scala| 400|        1|\n",
      "|    Spark| 500|        2|\n",
      "|    Kafka| 300|        3|\n",
      "|     Java| 350|        5|\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bookdata: Seq[(String, Int, Int)] = List((Scala,400,1), (Spark,500,2), (Kafka,300,3), (Java,350,5))\r\n",
       "book_df: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bookdata = Seq((\"Scala\", 400, 1),(\"Spark\", 500, 2),(\"Kafka\", 300, 3),(\"Java\", 350, 5))\n",
    "val book_df = bookdata.toDF(\"book_name\",\"cost\",\"writer_id\")\n",
    "book_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bff87",
   "metadata": {},
   "source": [
    "**1. Inner Join**\n",
    "The INNER JOIN returns the dataset which has the rows that have matching values in both the datasets i.e. value of the common field will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "12afa558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----------+---------+\n",
      "|book_name|cost|writer_id|writer_name|writer_id|\n",
      "+---------+----+---------+-----------+---------+\n",
      "|    Scala| 400|        1|     Martin|        1|\n",
      "|    Spark| 500|        2|    Zaharia|        2|\n",
      "|    Kafka| 300|        3|       Neha|        3|\n",
      "+---------+----+---------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterInner: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterInner = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"inner\")\n",
    "BookWriterInner.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e06be1",
   "metadata": {},
   "source": [
    "**2. Cross Join**\n",
    "The CROSS JOIN returns the dataset which is the number of rows in the first dataset multiplied by the number of rows in the second dataset. Such kind of result is called the Cartesian Product. \n",
    "Prerequisite: For using a cross join, spark.sql.crossJoin.enabled must be set to true.\n",
    "Otherwise, the exception will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "33ab5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "89a90bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----------+---------+\n",
      "|book_name|cost|writer_id|writer_name|writer_id|\n",
      "+---------+----+---------+-----------+---------+\n",
      "|    Scala| 400|        1|     Martin|        1|\n",
      "|    Scala| 400|        1|    Zaharia|        2|\n",
      "|    Scala| 400|        1|       Neha|        3|\n",
      "|    Scala| 400|        1|      James|        4|\n",
      "|    Spark| 500|        2|     Martin|        1|\n",
      "|    Spark| 500|        2|    Zaharia|        2|\n",
      "|    Spark| 500|        2|       Neha|        3|\n",
      "|    Spark| 500|        2|      James|        4|\n",
      "|    Kafka| 300|        3|     Martin|        1|\n",
      "|    Kafka| 300|        3|    Zaharia|        2|\n",
      "|    Kafka| 300|        3|       Neha|        3|\n",
      "|    Kafka| 300|        3|      James|        4|\n",
      "|     Java| 350|        5|     Martin|        1|\n",
      "|     Java| 350|        5|    Zaharia|        2|\n",
      "|     Java| 350|        5|       Neha|        3|\n",
      "|     Java| 350|        5|      James|        4|\n",
      "+---------+----+---------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterCross: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterCross = book_df.join(writer_df)\n",
    "BookWriterCross.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0a533",
   "metadata": {},
   "source": [
    "**3. Left Outer Join**\n",
    "The LEFT OUTER JOIN returns the dataset that has all rows from the left dataset,\n",
    "and the matched rows from the right dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cab41228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----------+---------+\n",
      "|book_name|cost|writer_id|writer_name|writer_id|\n",
      "+---------+----+---------+-----------+---------+\n",
      "|    Scala| 400|        1|     Martin|        1|\n",
      "|    Spark| 500|        2|    Zaharia|        2|\n",
      "|    Kafka| 300|        3|       Neha|        3|\n",
      "|     Java| 350|        5|       null|     null|\n",
      "+---------+----+---------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterLeft: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterLeft = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"leftouter\")\n",
    "BookWriterLeft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dcda4",
   "metadata": {},
   "source": [
    "**4. Right outer join**\n",
    "The RIGHT OUTER JOIN returns the dataset that has all rows from the right dataset, and the matched rows from the left dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ace6ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----------+---------+\n",
      "|book_name|cost|writer_id|writer_name|writer_id|\n",
      "+---------+----+---------+-----------+---------+\n",
      "|    Scala| 400|        1|     Martin|        1|\n",
      "|    Spark| 500|        2|    Zaharia|        2|\n",
      "|    Kafka| 300|        3|       Neha|        3|\n",
      "|     null|null|     null|      James|        4|\n",
      "+---------+----+---------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterRight: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterRight = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"rightouter\")\n",
    "BookWriterRight.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e9812",
   "metadata": {},
   "source": [
    "**5. Full Outer Join**\n",
    "The FULL OUTER JOIN returns the dataset that has all rows when there is a match in either the left or right dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dfce9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----------+---------+\n",
      "|book_name|cost|writer_id|writer_name|writer_id|\n",
      "+---------+----+---------+-----------+---------+\n",
      "|    Scala| 400|        1|     Martin|        1|\n",
      "|    Kafka| 300|        3|       Neha|        3|\n",
      "|     Java| 350|        5|       null|     null|\n",
      "|     null|null|     null|      James|        4|\n",
      "|    Spark| 500|        2|    Zaharia|        2|\n",
      "+---------+----+---------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterFull: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterFull = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"fullouter\")\n",
    "BookWriterFull.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309ec8d",
   "metadata": {},
   "source": [
    "**6. Left Semi Outer Join**\n",
    "The LEFT SEMI JOIN returns the dataset which has all rows from the left dataset having their correspondence in the right dataset. \n",
    "Unlike the LEFT OUTER JOIN, the returned dataset in LEFT SEMI JOIN contains only the columns from the left dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8a884226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+\n",
      "|book_name|cost|writer_id|\n",
      "+---------+----+---------+\n",
      "|    Scala| 400|        1|\n",
      "|    Spark| 500|        2|\n",
      "|    Kafka| 300|        3|\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterLeftSemi: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterLeftSemi = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"leftsemi\")\n",
    "BookWriterLeftSemi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb450bf",
   "metadata": {},
   "source": [
    "**7. Left Anti Join**\n",
    "The ANTI SEMI JOIN returns the dataset which has all the rows from the left dataset that donâ€™t have their matching in the right dataset. It also contains only the columns from the left dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b9cb0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+\n",
      "|book_name|cost|writer_id|\n",
      "+---------+----+---------+\n",
      "|     Java| 350|        5|\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookWriterLeftAnti: org.apache.spark.sql.DataFrame = [book_name: string, cost: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BookWriterLeftAnti = book_df.join(writer_df, book_df(\"writer_id\") === writer_df(\"writer_id\"), \"leftanti\")\n",
    "BookWriterLeftAnti.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a7077",
   "metadata": {},
   "source": [
    "**Q6)Store the given below dataset in a text file with the name stock_market.txt**\n",
    "\n",
    "2012-01-03,59.970001,61.060001,59.869999,60.330002,12668800,52.619234999999996,2012-01-04,60.209998999999996,60.349998,59.470001,59.709998999999996,9593300,52.078475,2012-01-05,59.349998,59.619999,58.369999,59.419998,12768200,51.825539,2012-01-06,59.419998,59.450001,58.869999,59.0,8069400,51.45922,2012-01-09,59.029999,59.549999,58.919998,59.18,6679300,51.616215000000004,2012-01-10,59.43,59.709998999999996,58.98,59.040001000000004,6907300,51.494109,2012-01-11,59.060001,59.529999,59.040001000000004,59.400002,6365600,51.808098,2012-01-12,59.790001000000004,60.0,59.400002,59.5,7236400,51.895315999999994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9df6c",
   "metadata": {},
   "source": [
    "a)Seperate the line into a new row after occurance of every 7th comma as a delimiter and display the result. There will be 7 columns named Date,Open,High,Low,Close,Volume,AdjClose should be tagged with the data. Make use of Case Class wherever necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef890211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_rdd: org.apache.spark.rdd.RDD[String] = C:\\Users\\SUPBHAUS\\Desktop\\Scala Training\\Stock_market.txt MapPartitionsRDD[320] at textFile at <console>:59\r\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stock_rdd=sc.textFile(\"C:\\\\Users\\\\SUPBHAUS\\\\Desktop\\\\Scala Training\\\\Stock_market.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be8b116a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df=stock_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "540063f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "368ed385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Stock\r\n",
       "import spark.implicits._\r\n",
       "stock_df: org.apache.spark.sql.DataFrame = [value: array<string>, date: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Stock(date: String, open: String, high: Double, low: Double, close: Double, volume: Long, adjClose: Double)\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val stock_df=df.withColumn(\"value\", regexp_replace(col(\"value\"), \"([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),\", \"$1,$2,$3,$4,$5,$6,$7\\n\"))\n",
    "  .withColumn(\"value\", explode(split(col(\"value\"), \"\\n\")))\n",
    "  .withColumn(\"value\", split(col(\"value\"), \",\")).as[Seq[String]].withColumn(\"date\",col(\"value\").getItem(0)).withColumn(\"open\",col(\"value\").getItem(1)).withColumn(\"high\",col(\"value\").getItem(2)).withColumn(\"low\",col(\"value\").getItem(3)).withColumn(\"close\",col(\"value\").getItem(4)).withColumn(\"volume\",col(\"value\").getItem(5)).withColumn(\"adjClose\",col(\"value\").getItem(6))\n",
    "  //.map(a => Stock(a.head, a(1), a(2).toDouble, a(3).toDouble, a(4).toDouble, a(5).toLong, a(6).toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "896dc39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|      date|              open|              high|               low|             close|  volume|          adjClose|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.select(col(\"date\"),col(\"open\"),col(\"high\"),col(\"low\"),col(\"close\"),col(\"volume\"),col(\"adjClose\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "18e2fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+---------+---------+---------+--------+------------------+\n",
      "|               value|      date|     open|     high|      low|    close|  volume|          adjClose|\n",
      "+--------------------+----------+---------+---------+---------+---------+--------+------------------+\n",
      "|[2012-01-03, 59.9...|2012-01-03|59.970001|61.060001|59.869999|60.330002|12668800|52.619234999999996|\n",
      "+--------------------+----------+---------+---------+---------+---------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49866e",
   "metadata": {},
   "source": [
    "##Q7) Given below is a comma delimited dataset. Store it in a CSV file.\n",
    "\n",
    "id,name,age,City\n",
    "1,Ajay,23,Kolkata\n",
    "2,Alok,45,Bangalore\n",
    "3,Ashish,34,Lucknow\n",
    "4,Abhishek,26,Delhi\n",
    "5,Amit,28,Nagpur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363f7ae",
   "metadata": {},
   "source": [
    "###a) Load the dataset in a dataframe and arrange it by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "762aa1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datadf: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datadf=spark.read.option(\"header\",true).csv(\"C:\\\\Users\\\\SUPBHAUS\\\\Desktop\\\\Scala Training\\\\DataEmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3f28b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---------+\n",
      "| id|    name|age|     City|\n",
      "+---+--------+---+---------+\n",
      "|  1|    Ajay| 23|  Kolkata|\n",
      "|  2|    Alok| 45|Bangalore|\n",
      "|  3|  Ashish| 34|  Lucknow|\n",
      "|  4|Abhishek| 26|    Delhi|\n",
      "|  5|    Amit| 28|   Nagpur|\n",
      "+---+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datadf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "64b9ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---------+\n",
      "| id|    name|age|     City|\n",
      "+---+--------+---+---------+\n",
      "|  1|    Ajay| 23|  Kolkata|\n",
      "|  4|Abhishek| 26|    Delhi|\n",
      "|  5|    Amit| 28|   Nagpur|\n",
      "|  3|  Ashish| 34|  Lucknow|\n",
      "|  2|    Alok| 45|Bangalore|\n",
      "+---+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datadf.orderBy(col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c10e6",
   "metadata": {},
   "source": [
    "### b) Hide the first 3 letters of name and make it confidential as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "490f4565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datadf1: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//REGEXP performs a pattern match of a string expression against a pattern. The pattern is supplied as an argument.\n",
    "//If the pattern finds a match in the expression, the function returns 1, else it returns 0. \n",
    "//If either expression or pattern is NULL, the function returns NULL\n",
    "\n",
    "val datadf1=datadf.withColumn(\"name\",regexp_replace(col(\"name\"),\"^...\",\"***\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0d3dce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---------+\n",
      "| id|    name|age|     City|\n",
      "+---+--------+---+---------+\n",
      "|  1|    ***y| 23|  Kolkata|\n",
      "|  2|    ***k| 45|Bangalore|\n",
      "|  3|  ***ish| 34|  Lucknow|\n",
      "|  4|***ishek| 26|    Delhi|\n",
      "|  5|    ***t| 28|   Nagpur|\n",
      "+---+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datadf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f75b",
   "metadata": {},
   "source": [
    "##Questionare:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2df19c",
   "metadata": {},
   "source": [
    "### 1.\tWhat are Case Classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16166b07",
   "metadata": {},
   "source": [
    "A **Case Class** is just like a regular class, which has a feature for modeling unchangeable data.\n",
    "--Pattern Matching:\n",
    "Pattern matching allows us to nicely decompose classes and write intuitive code.\n",
    "Itâ€™s definitely the coolest feature we have in case classes\n",
    "\n",
    "It is serializable. Case class canâ€™t inherit from another case class.\n",
    "It has a by default hashCode implementation.\n",
    "you can create objects of the Case Class even in the absence of the keyword new.\n",
    "Companion objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1f99d9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class CovidCountryStats\r\n"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Letâ€™s start by defining an example case class:\n",
    "//By default, all constructor parameters are public and immutable.\n",
    "\n",
    "case class CovidCountryStats(countryCode: String, deaths: Int, confirmedCases: Int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "89cb09a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "covidPL: CovidCountryStats = CovidCountryStats(PL,776,15366)\r\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//We can create an instance of a case class in a short and concise way:\n",
    "\n",
    "val covidPL = CovidCountryStats(\"PL\", 776, 15366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9c35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d744d8f4",
   "metadata": {},
   "source": [
    "### 2.\tWhat is the difference between var and value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c603",
   "metadata": {},
   "source": [
    "**val** means value and **var** means variable\"\n",
    "\n",
    "Scala has two types of variables:\n",
    "\n",
    "- val creates an immutable variable (like final in Java).\n",
    "- var creates a mutable variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f6bba43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s: String = hello\r\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = \"hello\"   // immutable//Variable reassignment is not possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "745e5c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "66: error: reassignment to val\r",
     "output_type": "error",
     "traceback": [
      "<console>:66: error: reassignment to val\r",
      "       s = \"Hii\"\r",
      "         ^\r",
      ""
     ]
    }
   ],
   "source": [
    "s = \"Hii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "var i = 42        // mutable//Variable reassignment is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b5841",
   "metadata": {},
   "source": [
    "### 3.Describe Exception Handling in Scala?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd842ab",
   "metadata": {},
   "source": [
    "Exception handling in Scala is implemented differently, but it behaves exactly like Java and works seamlessly with existing Java libraries. \n",
    "In scala, All exceptions are unchecked. \n",
    "there is no concept of checked exception Scala facilitates a great deal of flexibility in terms of the ability to choose whether to catch an exception.\n",
    "\n",
    "The Throwing Exceptions:\n",
    "Throwing an exception. It looks same as in Java. we create an exception object and then we throw it by using throw keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e4da0",
   "metadata": {},
   "source": [
    "Syntax:\n",
    "\n",
    "throw new ArithmeticException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c82bc1",
   "metadata": {},
   "source": [
    "The try/catch Construct:\n",
    "The try/catch construct is different in Scala than in Java, In Scala it is an expression.\n",
    "The exception in Scala and that results in a value can be pattern matched in the catch block instead of providing a separate catch clause for each different exception.\n",
    "Here is an example of exception Handling using the conventional try-catch block in Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af13a1",
   "metadata": {},
   "source": [
    "If we want some part of our code to execute irrespective of how the expression terminates we can use a finally block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b176f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.IOException\n",
    "\n",
    "object GFG\n",
    "{\n",
    "    // Main method\n",
    "    def main(args:Array[String])\n",
    "    {\n",
    "        try\n",
    "        {\n",
    "            var N = 5/0\n",
    "        }\n",
    "        catch\n",
    "        {\n",
    "\n",
    "            case i: IOException =>\n",
    "            {\n",
    "                println(\"IOException occurred.\")\n",
    "            }\n",
    "            case a : ArithmeticException =>\n",
    "            {\n",
    "                println(\"Arithmetic Exception occurred.\")\n",
    "            }\n",
    "\n",
    "        }\n",
    "        finally\n",
    "        {\n",
    "            // Finally block will execute \n",
    "            println(\"This is final block.\")\n",
    "        }\n",
    "\n",
    "     }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcb7e",
   "metadata": {},
   "source": [
    "### 4.What is a closure in Scala?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c68ff3",
   "metadata": {},
   "source": [
    "Scala Closures are functions which uses one or more free variables and the return value of this function is dependent of these variable. \n",
    "The free variables are defined outside of the Closure Function and is not included as a parameter of this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// defined the value of p as 10\n",
    "val p = 10\n",
    "\n",
    "// define this closure.\n",
    "def example(a:Double) = a*p / 100\n",
    "example(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447a286",
   "metadata": {},
   "source": [
    "what closure function does is, that it takes the most recent state of the free variable and changes the value of the closure function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df5bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316aa2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696447a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
